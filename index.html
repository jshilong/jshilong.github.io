<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
    <script type="text/javascript" src="js/hidebib.js"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-7580334-2');
    </script>

    <title>Shilong Zhang</title>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">

    <meta name="author" content="Shilong Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <!-- <link rel="icon" type="image/png" href="images/JHU_icon.jpg"> -->
</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:70%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Shilong Zhang(张士龙)</name>
                                    </p>

                                    <p> Shilong, presently in his second year (2023-present) as a Ph.D. student at the
                                        Department of Computer Science, <a href="https://www.cs.hku.hk/"></a>The
                                        University
                                        of Hong Kong (HKU)</a>, is being
                                        mentored by <a href="http://luoping.me/">Prof. Ping Luo</a>. In his previous
                                        stint, he was an integral part of the
                                        team headed by <a href="http://chenkai.site/"> Kai Chen</a> while serving as a
                                        core developer for <a
                                            href="https://github.com/open-mmlab/mmdetection">MMDetection</a> and <a
                                            href="https://github.com/open-mmlab/mmcv">MMCV</a>.
                                    </p>
                                    <p> He completed his Bachelor's degree from the <a
                                            href="https://www.ustc.edu.cn">University of Science and Technology
                                            of China (USTC)</a> in 2019, distinguishing himself as one of the top
                                        <b>4%</b> <a
                                            href="https://www.teach.ustc.edu.cn/?attachment_id=9884">outstanding
                                            graduates</a>.
                                    </p>
                                    <p> At present, Shilong's research is primarily focused on building efficient vision
                                        generative models and algorithms.
                                        He welcomes intellectual conversations about these areas of study.</p>





                                    <p style="text-align:center">
                                        <a href="mailto:2392587229zsl@gmail.com">Email</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?hl=zh-CN&user=s1NMu_UAAAAJ">Google
                                            Scholar</a> &nbsp/&nbsp
                                        <!-- <a href="">CV</a> &nbsp/&nbsp -->
                                        <!-- <a href="https://scholar.google.com/citations?user=X3vVZPcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp -->
                                        <a href="https://github.com/jshilong">Github</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:300%;max-width:300%">
                                    <a href="images/ShilongZhang.jpg"><img
                                            style="width:110%;max-width:110%;border-radius:15%" alt="profile photo"
                                            src="images/ShilongZhang.jpg" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <hr>


                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:10px;width:100%;vertical-align:middle">
                                    <heading>Recent News</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <table
                                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                <tbody>
                                    <tr>
                                        <ul>
                                            <li> <b>[2025/0/10]</b> We present <a
                                                    href="https://jshilong.github.io/flashvideo-page/"> FlashVideo</a>,
                                                an efficient
                                                paradigm for text-to-video generation.</li>

                                            <li> <b>[2024/3/26]</b> We propose <a
                                                    href="https://jshilong.github.io/flashface-page/"> FlashFace</a>
                                                that can generate high ID fidelity images in seconds. </li>
                                            <li> <b>[2023/7/7]</b> We present a vision and language model named <a
                                                    href="https://github.com/jshilong/GPT4RoI"> GPT4RoI</a> to do
                                                region-level image understanding.</li>
                                            <li> <b>[2023/4/26]</b> We present a vision and language model named <a
                                                    href="https://github.com/open-mmlab/Multimodal-GPT">
                                                    MultiModal-GPT</a> . </li>
                                            <li> <b>[2023/3/20]</b> Two papers was accepted by <b>CVPR 2023.</b> DDQ
                                                DETR achieve 52.1 AP with R-50 backbone within 12 epochs. </li>
                                            <li> <b>[2022/3/15]</b> One paper was accepted by <b>CVPR 2022.</b> </li>
                                            <li> <b>[2021/11/27]</b> We release <a
                                                    href="https://github.com/open-mmlab/mmfewshot"> MMFewShot</a>, an
                                                open source few shot learning toolbox based on PyTorch. </li>
                                            <li> <b>[2021/5/8]</b> One paper was accepted by <b>ICML 2021.</b> </li>
                                            <li> <b>[2020/2/24]</b> One paper was accepted by <b>CVPR 2020.</b></li>
                                            <li> <b>[2019/6/28]</b> Awarded as outstanding graduates by USTC. </li>


                                        </ul>
                </td>
            </tr>
        </tbody>
    </table>
    <hr>


    <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Publications</heading>
                </td>

            </tr>
        </tbody>
    </table>




    <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/flashvideo.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <a href="">[1] FlashVideo: Flowing Fidelity to Detail for Efficient High-Resolution Video
                            Generation
                        </a> &nbsp&nbsp
                        <br> <b>Shilong Zhang*</b>, Webo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang,
                        Zehuan Yuan, Binyue Peng, Ping Luo

                        <br>
                        <font color="black"> (a) Dividing the process into prompt fidelity and
                            quality enhancement stages, delivering a stunning reduction in DiT's
                            computational load .
                            (b) Enableing users to preview the initial output and accordingly adjust the prompt before
                            committing to full-resolution
                            generation, thereby significantly reducing wait times and enhancing commercial
                            viability.<br><a href="https://github.com/FoundationVision/FlashVideo"> Code has been
                                released
                                at this repo !</a><br> </font>
                        <br>
                    </p>

                </td>
            </tr>
            <!--xie2020advprop-->






            </tr>
        </tbody>
    </table>













    <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/arch-minv2.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <a href="">[2] FlashFace: Human Image Personalization with High-fidelity Identity Preservation
                        </a> &nbsp&nbsp
                        <br> <b>Shilong Zhang</b>, Lianghua Huang, Xi Chen, Yifei Zhang, Zhi-Fan Wu, Yutong Feng, Wei
                        Wang, Yujun Shen, Yu Liu, Ping Luo

                        <br>
                        <font color="black">This work presents FlashFace, a practical tool with which users can easily
                            personalize their own photos on the fly by providing one or a few reference face images and
                            a text prompt <br><a href="https://github.com/ali-vilab/FlashFace"> Code has been released
                                at this repo !</a><br> </font>
                        <br>
                    </p>

                </td>
            </tr>
            <!--xie2020advprop-->






            </tr>
        </tbody>
    </table>
    <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/gpt4roi.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <a href="./images/gpt4roi.pdf">[3] GPT4RoI: Instruction Tuning Large Language Model on
                            Region-of-Interest </a> &nbsp&nbsp
                        <b>Shilong Zhang*</b>, Peize Sun*, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen,
                        Ping Luo
                        <br>
                        <em>(* Equal contribution) </em>,
                        <br>

                        <font color="black">We present a vision and language model named GPT4RoI to do region-level
                            image understanding. <br> <a href="https://github.com/jshilong/GPT4RoI"> Code has been
                                released at this repo !</a> <br> </font>
                        <br>
                    </p>

                </td>
            </tr>
            <!--xie2020advprop-->



            </tr>
        </tbody>
    </table>
    <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/gpt.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <a href="">[4] MultiModal-GPT: A Vision and Language Model for Dialogue with Humans </a>
                        &nbsp&nbsp
                        <br> Tao Gong*, Chengqi Lyu*, <b>Shilong Zhang*</b>, Yudong Wang*, Miao Zheng*, Qian Zhao*,
                        Kuikun Liu*, Wenwei Zhang*, Ping Luo, Kai Chen
                        <br>
                        <em>(* random order) </em>,
                        <br>

                        <font color="black">We present a vision and language model named MultiModal-GPT to conduct
                            multi-round dialogue with humans. <br> <a
                                href="https://github.com/open-mmlab/Multimodal-GPT"> Code has been released at this repo
                                !</a> <br> </font>
                        <br>
                    </p>

                </td>
            </tr>
            <!--xie2020advprop-->



            </tr>
        </tbody>
    </table>
    <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/ddq.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <a href="">[5] Dense Distinct Query for End-to-End Object Detection </a> &nbsp&nbsp


                        <br>
                        <b>Shilong Zhang*</b>, Xinjiang Wang*, Jiaqi Wang, Jiangmiao Pang, Chengqi Lyu, Wenwei Zhang,
                        Ping Luo, Kai Chen
                        <br>
                        <em>CVPR2023(* Equal contribution) </em>,
                        <br>

                        <font color="black">DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12 epochs using a
                            ResNet-50 backbone, outperforming all existing detectors in the same setting. <br> <a
                                href=https://github.com/jshilong/DDQ> Code has been released at this repo !</a> <br>
                        </font>
                        <br>
                    </p>

                </td>
            </tr>
            <!--xie2020advprop-->


            <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/c_teacher.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <a href="https://arxiv.org/abs/2209.01589">[6] Consistent-Teacher: Towards Reducing Inconsistent
                            Pseudo-targets in Semi-supervised Object Detection </a> &nbsp&nbsp

                        <br> Xinjiang Wang*, Xingyi Yang*, <b>Shilong Zhang</b>, Yijiang Li, Litong Feng, Shijie Fang,
                        Chengqi Lyu, Kai Chen, Wayne Zhang

                        <br>
                        <em>CVPR2023 (* Equal contribution) </em>,
                        <br>
                        <font color="black">It achieves 40.0 mAP with ResNet-50 backbone given only 10% of annotated
                            MS-COCO data, which surpasses previous baselines using pseudo labels by around 3 mAP. When
                            trained on fully annotated MS-COCO with additional unlabeled
                            data, the performance further increases to 47.2 mAP. <br> <a
                                href=https://github.com/Adamdad/ConsistentTeacher> Code has been released !</a> <br>
                        </font>
                        <br>


                </td>
            </tr>
            <!--xie2020advprop-->



            <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/grouprcnn.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <a href="https://arxiv.org/abs/2205.05920">[7] Group R-CNN for Point-based Weakly
                            Semi-supervised Object Detection </a> &nbsp&nbsp

                        <br>
                        <b>Shilong Zhang*</b>, Zhuoran Yu*, Liyang Liu*, Xinjiang Wang, Aojun Zhou, Kai Chen
                        <br>
                        <em>CVPR2022 (* Equal contribution) </em>,
                        <br>
                        <font color="black">We study the problem of weakly semi-supervised object detection with points
                            (WSSOD-P). Group R-CNN significantly outperforms the prior method Point DETR by 3.9 mAP with
                            5% well-labeled images. <br> <a href=https://github.com/jshilong/GroupRCNN> Code has been
                                released !</a> <br></font>
                        <br>


                </td>
            </tr>
            <!--xie2020advprop-->



            <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/pruning.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <a href="https://arxiv.org/abs/2108.00708">[8] Group Fisher Pruning for Practical Network
                            Compression </a> &nbsp&nbsp


                        <br> Liyang Liu*, <b>Shilong Zhang*</b>,Zhanghui Kuang,Jing-Hao Xue ,Aojun Zhou
                        <br>
                        <em>ICML2021 (* Equal contribution) </em>,
                        <br>
                        <font color="black">We present a general channel pruning framework for complicated structures !
                            <br> <a href=https://github.com/jshilong/FisherPruning.git> Code has been released !</a>
                            <br>
                        </font>
                        <br>
                    </p>

                </td>
            </tr>
            <!--xie2020advprop-->


            <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/sepc.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <a href="https://arxiv.org/abs/2005.03101">[9] Scale-equalizing Pyramid Convolution for object
                            detection </a> &nbsp&nbsp

                        <!-- <a herf="images/sepc.pdf">
                    <papertitle>[1] Scale-equalizing Pyramid Convolution for object detection  </papertitle>
                  </a> -->
                        <br> Xinjiang Wang*, <b>Shilong Zhang*</b> , Zhuoran Yu, Litong Zhang, Wayne Zhang
                        <br>
                        <em>CVPR2020 (* Equal contribution)</em>,
                        <br>
                        <font color="black">We proposed a scale-equalizing pyramid convolution method that relaxes the
                            discrepancy between the feature pyramid and the gaussian pyramid. The module boosts the
                            performance about 3.5 mAP in single-stage object detection
                            with negligible inference time. <br> <a href=https://github.com/jshilong/SEPC> Code has been
                                released !</a> <br> </font>
                        <br>
                    </p>

                </td>
            </tr>
            <!--xie2020advprop-->


        </tbody>
    </table>


    <hr>

    <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td width=30% align="center">
                    <script type="text/javascript" id="clustrmaps"
                        src="//cdn.clustrmaps.com/map_v2.js?d=YhKG6U66qGxhpkw9wNq2inR7zEgWsQgm_B10AAnjACw&cl=ffffff&w=a"></script>
                </td>
                <td style="padding:10px">
                    <br>
                    <p style="text-align:right;">Stolen from <a href="https://jonbarron.info/">Jon Barron</a></p>
                </td>
            </tr>
        </tbody>
    </table>
    </td>
    </tr>
    </table>

    <script xml:space="preserve" language="JavaScript">
        hideallbibs();
    </script>


    <!-- Google Analytics -->
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
        ga('create', 'UA-131560165-1', 'auto');
        ga('send', 'pageview');
    </script>


    <!-- End Google Analytics -->



</body>

</html>